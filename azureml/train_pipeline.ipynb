{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = 'iris-classification-training'\n",
    "upload_sample_data = False\n",
    "register_data = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import required packages to build the pipeline artifact\n",
    "from azureml.core import Experiment, Dataset\n",
    "from azureml.core.compute import AmlCompute, ComputeTarget, DatabricksCompute\n",
    "from azureml.core.datastore import Datastore\n",
    "from azureml.core.runconfig import CondaDependencies, RunConfiguration\n",
    "from azureml.data.data_reference import DataReference\n",
    "from azureml.pipeline.core import Pipeline, PipelineData, PortDataReference\n",
    "from azureml.pipeline.steps import PythonScriptStep, DatabricksStep\n",
    "from azureml.core.model import Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from azureml.core import Workspace\n",
    "\n",
    "ws = Workspace.from_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.compute_target import ComputeTargetException\n",
    "\n",
    "aml_compute_target = \"cpu-cluster\"\n",
    "try:\n",
    "    aml_compute = AmlCompute(ws, aml_compute_target)\n",
    "    print(\"found existing compute target.\")\n",
    "except ComputeTargetException:\n",
    "    print(\"creating new compute target\")\n",
    "    \n",
    "    provisioning_config = AmlCompute.provisioning_configuration(vm_size = \"STANDARD_D2_V2\",\n",
    "                                                                min_nodes = 0, \n",
    "                                                                max_nodes = 4)    \n",
    "    aml_compute = ComputeTarget.create(ws, aml_compute_target, provisioning_config)\n",
    "    aml_compute.wait_for_completion(show_output=True, min_node_count=None, timeout_in_minutes=20)\n",
    "    \n",
    "print(\"Aml Compute attached\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the default blob store (Datastore) for the Azure ML workspace\n",
    "ds = ws.get_default_datastore()\n",
    "print(\"Default Blobstore's name: {}\".format(ds.name))\n",
    "print(\"Default Blobstore's container name: {}\".format(ds.container_name))"
   ]
  },
  {
   "source": [
    "Upload and register the iris data to Azure ML to be used in pipelines"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upload iris data\n",
    "if upload_sample_data:\n",
    "    ds.upload_files(['../sample_data.csv'], target_path='/data', overwrite=True, show_progress=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if register_data:\n",
    "    # create target dataset \n",
    "    registereddata = Dataset.Tabular.from_delimited_files(ds.path('/data/sample_data.csv'))\n",
    "    # NO TIMESTAMP COLUMN EXISTS\n",
    "    #target = target.with_timestamp_columns('datetime')\n",
    "    # register the target dataset\n",
    "    registereddata = registereddata.register(ws, 'iris-data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset.get_by_name(ws,name='iris-data',version='latest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_ref = DataReference(\n",
    "    datastore=ds,\n",
    "    data_reference_name='irisdata',\n",
    "    path_on_datastore=\"data/sample_data.csv\")\n",
    "print(\"DataReference object created\")\n",
    "\n",
    "\n",
    "model_output = PipelineData(\"model_output\",datastore=ds)\n",
    "print(\"PipelineData object created for models\")"
   ]
  },
  {
   "source": [
    "#### Create Pipeline Steps to be executed each time the pipeline runs"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.runconfig import RunConfiguration\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "from azureml.core.runconfig import DEFAULT_CPU_IMAGE\n",
    "\n",
    "# create a new runconfig object\n",
    "run_config = RunConfiguration()\n",
    "\n",
    "# enable Docker \n",
    "run_config.environment.docker.enabled = True\n",
    "# set Docker base image to the default CPU-based image\n",
    "run_config.environment.docker.base_image = DEFAULT_CPU_IMAGE\n",
    "# use conda_dependencies.yml to create a conda environment in the Docker image for execution\n",
    "run_config.environment.python.user_managed_dependencies = False\n",
    "\n",
    "# specify CondaDependencies obj\n",
    "run_config.environment.python.conda_dependencies = CondaDependencies.create(\n",
    "    pip_packages=['azureml-sdk','sklearn', 'scipy', 'numpy', 'pandas'],\n",
    "    conda_packages=['matplotlib'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the transformation script to produce the intermediate data that will go to the inferencing step\n",
    "trainingScript = PythonScriptStep(\n",
    "    script_name=\"iris_supervised_model.py\", \n",
    "    inputs=[dataset_ref],\n",
    "    outputs=[model_output],\n",
    "    compute_target=aml_compute, \n",
    "    source_directory=\".\",\n",
    "    runconfig=run_config\n",
    ")\n",
    "print(\"trainingStep created\")"
   ]
  },
  {
   "source": [
    "### Using the output\n",
    "In the previous PythonScriptStep, a PipelineOutputFileDataset was created as an output and assigned to \"model_output\". Doc is here: https://docs.microsoft.com/en-us/python/api/azureml-pipeline-core/azureml.pipeline.core.pipeline_output_dataset.pipelineoutputfiledataset?view=azure-ml-py"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the transformation script to produce the intermediate data that will go to the inferencing step\n",
    "registerModelStep = PythonScriptStep(\n",
    "    script_name=\"register_model.py\", \n",
    "    arguments=[\"--model_name\", \"iris_classifier_model\",\"--training_step_name\",\"iris_supervised_model.py\"],\n",
    "    inputs=[dataset_ref,model_output],\n",
    "    #outputs=[model_output],\n",
    "    compute_target=aml_compute, \n",
    "    source_directory=\".\",\n",
    "    runconfig=run_config\n",
    ")\n",
    "print(\"registerModelStep created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_train_pipeline = Pipeline(workspace=ws, steps=[trainingScript,registerModelStep])\n",
    "print (\"Pipeline is built\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = Experiment(ws,experiment_name)\n",
    "exp.set_tags({'automl':'no','working':'no'})\n",
    "\n",
    "pipeline_run1 = exp.submit(iris_train_pipeline)\n",
    "print(\"Pipeline is submitted for execution\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_run1.wait_for_completion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Steps\n",
    "model_type=None\n",
    "model_accuracy=None\n",
    "run_id=0\n",
    "for step in pipeline_run1.get_steps():\n",
    "    print(\"Outputs of step \" + step.name)\n",
    "    \n",
    "    # Get a dictionary of StepRunOutputs with the output name as the key \n",
    "    output_dict = step.get_outputs()\n",
    "\n",
    "    if step.name == 'iris_supervised_model.py':\n",
    "        #step.download_file('model_output',output_file_path='.')\n",
    "        model_type = step.get_properties()['best_model']\n",
    "        model_accuracy = float(step.get_properties()['accuracy'])\n",
    "        run_id = step.id\n",
    "    for name, output in output_dict.items():\n",
    "        \n",
    "        output_reference = output.get_port_data_reference() # Get output port data reference\n",
    "        print(\"\\tname: \" + name)\n",
    "        print(\"\\tdatastore: \" + output_reference.datastore_name)\n",
    "        print(\"\\tpath on datastore: \" + output_reference.path_on_datastore)\n",
    "\n",
    "            \n"
   ]
  },
  {
   "source": [
    "## Model Registration\n",
    "The below code is an example of how to register a model, in the automated code, this is completed in register_model.py instead of train_pipeline.py"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from azureml.core import Model\n",
    "from azureml.core.resource_configuration import ResourceConfiguration\n",
    "import sklearn\n",
    "\n",
    "model_name = 'iris-model'\n",
    "model_path = 'model_output'\n",
    "\n",
    "try:\n",
    "    model = Model(ws, model_name)\n",
    "    current_accuracy = float(model.properties[\"accuracy\"])\n",
    "except:\n",
    "    current_accuracy = 0\n",
    "\n",
    "print(\"current accuracy\",current_accuracy)\n",
    "if model_accuracy > current_accuracy:\n",
    "    print(\"model is better\")\n",
    "    model = Model.register(workspace=ws,\n",
    "                       model_name=model_name,                # Name of the registered model in your workspace.\n",
    "                       model_path=model_path,  # Local file to upload and register as a model.\n",
    "                       model_framework=Model.Framework.SCIKITLEARN,  # Framework used to create the model.\n",
    "                       model_framework_version=sklearn.__version__,  # Version of scikit-learn used to create the model.\n",
    "                       sample_input_dataset=dataset,\n",
    "                       #sample_output_dataset=output_dataset,\n",
    "                       resource_configuration=ResourceConfiguration(cpu=1, memory_in_gb=0.5),\n",
    "                       description='basic iris classification',\n",
    "                       tags={'quality': 'good', 'type': 'classification'})\n",
    "    model.add_properties({\"accuracy\":model_accuracy,\"model_type\":model_type})\n",
    "    model.experiment_name=experiment_name\n",
    "    model.run_id = run_id\n",
    "\n"
   ]
  },
  {
   "source": [
    "## Pipeline Publish and Schedule\n",
    "train_pipeline.py needs to be modified to perform publishing (change pipeline=True) to be published from the automated run. The below cells can be performed interactively after the pipeline above is submitted if desired"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "published_pipeline = iris_train_pipeline.publish(name=\"iris_training_demo\", description=\"Iris Classification Demo\", continue_on_step_failure=True)\n",
    "published_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.pipeline.core.schedule import ScheduleRecurrence, Schedule\n",
    "\n",
    "recurrence = ScheduleRecurrence(frequency=\"Day\", interval=1, hours=[22], minutes=[30]) # Runs every day at 10:30pm\n",
    "\n",
    "schedule = Schedule.create(workspace=ws, name=\"iris_training_demo_schedule\",\n",
    "                           pipeline_id=published_pipeline.id, \n",
    "                           experiment_name='iris_training_demo_daily_schedule_run',\n",
    "                           recurrence=recurrence,\n",
    "                           wait_for_provisioning=True,\n",
    "                           description=\"iris training demo daily Schedule Run\")"
   ]
  }
 ]
}