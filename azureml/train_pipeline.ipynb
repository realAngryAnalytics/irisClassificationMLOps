{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = 'iris-classification-training'\n",
    "upload_sample_data = False\n",
    "register_data = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Failure while loading azureml_run_type_providers. Failed to load entrypoint automl = azureml.train.automl.run:AutoMLRun._from_run_dto with exception (cryptography 2.9.2 (c:\\program files (x86)\\microsoft visual studio\\shared\\anaconda3_64\\lib\\site-packages), Requirement.parse('cryptography<4.0.0,>=3.3.1; extra == \"crypto\"'), {'PyJWT'}).\n"
     ]
    }
   ],
   "source": [
    "#import required packages to build the pipeline artifact\n",
    "from azureml.core import Experiment, Dataset\n",
    "from azureml.core.compute import AmlCompute, ComputeTarget, DatabricksCompute\n",
    "from azureml.core.datastore import Datastore\n",
    "from azureml.core.runconfig import CondaDependencies, RunConfiguration\n",
    "from azureml.data.data_reference import DataReference\n",
    "from azureml.pipeline.core import Pipeline, PipelineData, PortDataReference\n",
    "from azureml.pipeline.steps import PythonScriptStep, DatabricksStep\n",
    "from azureml.core.model import Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Warning: Falling back to use azure cli login credentials.\nIf you run your code in unattended mode, i.e., where you can't give a user input, then we recommend to use ServicePrincipalAuthentication or MsiAuthentication.\nPlease refer to aka.ms/aml-notebook-auth for different authentication mechanisms in azureml-sdk.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from azureml.core import Workspace\n",
    "\n",
    "ws = Workspace.from_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "found existing compute target.\nAml Compute attached\n"
     ]
    }
   ],
   "source": [
    "from azureml.core.compute_target import ComputeTargetException\n",
    "\n",
    "aml_compute_target = \"cpu-cluster\"\n",
    "try:\n",
    "    aml_compute = AmlCompute(ws, aml_compute_target)\n",
    "    print(\"found existing compute target.\")\n",
    "except ComputeTargetException:\n",
    "    print(\"creating new compute target\")\n",
    "    \n",
    "    provisioning_config = AmlCompute.provisioning_configuration(vm_size = \"STANDARD_D2_V2\",\n",
    "                                                                min_nodes = 0, \n",
    "                                                                max_nodes = 4)    \n",
    "    aml_compute = ComputeTarget.create(ws, aml_compute_target, provisioning_config)\n",
    "    aml_compute.wait_for_completion(show_output=True, min_node_count=None, timeout_in_minutes=20)\n",
    "    \n",
    "print(\"Aml Compute attached\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Default Blobstore's name: workspaceblobstore\nDefault Blobstore's container name: azureml-blobstore-3ebbce3f-1760-43ec-942d-7ed495b50dbe\n"
     ]
    }
   ],
   "source": [
    "# Getting the default blob store (Datastore) for the Azure ML workspace\n",
    "ds = ws.get_default_datastore()\n",
    "print(\"Default Blobstore's name: {}\".format(ds.name))\n",
    "print(\"Default Blobstore's container name: {}\".format(ds.container_name))"
   ]
  },
  {
   "source": [
    "Upload and register the iris data to Azure ML to be used in pipelines"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upload iris data\n",
    "if upload_sample_data:\n",
    "    ds.upload_files(['../sample_data.csv'], target_path='/data', overwrite=True, show_progress=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if register_data:\n",
    "    # create target dataset \n",
    "    registereddata = Dataset.Tabular.from_delimited_files(ds.path('/data/sample_data.csv'))\n",
    "    # NO TIMESTAMP COLUMN EXISTS\n",
    "    #target = target.with_timestamp_columns('datetime')\n",
    "    # register the target dataset\n",
    "    registereddata = registereddata.register(ws, 'iris-data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset.get_by_name(ws,name='iris-data',version='latest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "DataReference object created\nPipelineData object created for models\n"
     ]
    }
   ],
   "source": [
    "dataset_ref = DataReference(\n",
    "    datastore=ds,\n",
    "    data_reference_name='irisdata',\n",
    "    path_on_datastore=\"data/sample_data.csv\")\n",
    "print(\"DataReference object created\")\n",
    "\n",
    "\n",
    "model_output = PipelineData(\"model_output\",datastore=ds)\n",
    "print(\"PipelineData object created for models\")"
   ]
  },
  {
   "source": [
    "#### Create Pipeline Steps to be executed each time the pipeline runs"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.runconfig import RunConfiguration\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "from azureml.core.runconfig import DEFAULT_CPU_IMAGE\n",
    "\n",
    "# create a new runconfig object\n",
    "run_config = RunConfiguration()\n",
    "\n",
    "# enable Docker \n",
    "run_config.environment.docker.enabled = True\n",
    "# set Docker base image to the default CPU-based image\n",
    "run_config.environment.docker.base_image = DEFAULT_CPU_IMAGE\n",
    "# use conda_dependencies.yml to create a conda environment in the Docker image for execution\n",
    "run_config.environment.python.user_managed_dependencies = False\n",
    "\n",
    "# specify CondaDependencies obj\n",
    "run_config.environment.python.conda_dependencies = CondaDependencies.create(\n",
    "    pip_packages=['azureml-sdk','sklearn', 'scipy', 'numpy', 'pandas'],\n",
    "    conda_packages=['matplotlib'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "trainingStep created\n"
     ]
    }
   ],
   "source": [
    "# run the transformation script to produce the intermediate data that will go to the inferencing step\n",
    "trainingScript = PythonScriptStep(\n",
    "    script_name=\"iris_supervised_model.py\", \n",
    "    #arguments=[\"--input_data_1\", nevada_booking_all_time]\n",
    "    inputs=[dataset_ref],\n",
    "    outputs=[model_output],\n",
    "    compute_target=aml_compute, \n",
    "    source_directory=\".\",\n",
    "    runconfig=run_config\n",
    ")\n",
    "print(\"trainingStep created\")"
   ]
  },
  {
   "source": [
    "### Using the output\n",
    "In the previous PythonScriptStep, a PipelineOutputFileDataset was created as an output and assigned to \"model_output\". Doc is here: https://docs.microsoft.com/en-us/python/api/azureml-pipeline-core/azureml.pipeline.core.pipeline_output_dataset.pipelineoutputfiledataset?view=azure-ml-py"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "registerModelStep created\n"
     ]
    }
   ],
   "source": [
    "# run the transformation script to produce the intermediate data that will go to the inferencing step\n",
    "registerModelStep = PythonScriptStep(\n",
    "    script_name=\"register_model.py\", \n",
    "    arguments=[\"--model_name\", \"iris_classifier_model\",\"--training_step_name\",\"iris_supervised_model.py\"],\n",
    "    inputs=[dataset_ref,model_output],\n",
    "    #outputs=[model_output],\n",
    "    compute_target=aml_compute, \n",
    "    source_directory=\".\",\n",
    "    runconfig=run_config\n",
    ")\n",
    "print(\"registerModelStep created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Pipeline is built\n"
     ]
    }
   ],
   "source": [
    "iris_train_pipeline = Pipeline(workspace=ws, steps=[trainingScript,registerModelStep])\n",
    "print (\"Pipeline is built\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Created step iris_supervised_model.py [25f60fd3][79b5a082-9c9b-49d4-aec1-8d2a51085815], (This step will run and generate new outputs)\n",
      "Created step register_model.py [30ca188d][4618d0bd-7fcc-4154-8214-dfe9b96f36f9], (This step will run and generate new outputs)\n",
      "Using data reference irisdata for StepId [ae7d6e0d][77fa1314-ea57-4a57-ab61-5f2222c7f6c1], (Consumers of this data are eligible to reuse prior runs.)\n",
      "Using data reference irisdata for StepId [1038e8ad][77fa1314-ea57-4a57-ab61-5f2222c7f6c1], (Consumers of this data are eligible to reuse prior runs.)\n",
      "Submitted PipelineRun b87d6bea-24ae-491e-845f-ccb546f20403\n",
      "Link to Azure Machine Learning Portal: https://ml.azure.com/experiments/iris-classification-training/runs/b87d6bea-24ae-491e-845f-ccb546f20403?wsid=/subscriptions/31e77061-7c45-4325-a0ec-1d348d195b23/resourcegroups/Synapse-WS-L400/workspaces/amlworkspacesjh\n",
      "Pipeline is submitted for execution\n"
     ]
    }
   ],
   "source": [
    "exp = Experiment(ws,experiment_name)\n",
    "exp.set_tags({'automl':'no','working':'no'})\n",
    "\n",
    "pipeline_run1 = exp.submit(iris_train_pipeline)\n",
    "print(\"Pipeline is submitted for execution\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_run1.wait_for_completion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Outputs of step iris_supervised_model.py\n",
      "\tname: model_output\n",
      "\tdatastore: workspaceblobstore\n",
      "\tpath on datastore: azureml/e0c7b39f-49d0-4e2c-9071-0aa86d7d8ccf/model_output\n"
     ]
    }
   ],
   "source": [
    "# Get Steps\n",
    "model_type=None\n",
    "model_accuracy=None\n",
    "run_id=0\n",
    "for step in pipeline_run1.get_steps():\n",
    "    print(\"Outputs of step \" + step.name)\n",
    "    \n",
    "    # Get a dictionary of StepRunOutputs with the output name as the key \n",
    "    output_dict = step.get_outputs()\n",
    "\n",
    "    if step.name == 'iris_supervised_model.py':\n",
    "        #step.download_file('model_output',output_file_path='.')\n",
    "        model_type = step.get_properties()['best_model']\n",
    "        model_accuracy = float(step.get_properties()['accuracy'])\n",
    "        run_id = step.id\n",
    "    for name, output in output_dict.items():\n",
    "        \n",
    "        output_reference = output.get_port_data_reference() # Get output port data reference\n",
    "        print(\"\\tname: \" + name)\n",
    "        print(\"\\tdatastore: \" + output_reference.datastore_name)\n",
    "        print(\"\\tpath on datastore: \" + output_reference.path_on_datastore)\n",
    "\n",
    "            \n"
   ]
  },
  {
   "source": [
    "## Model Registration\n",
    "The below code is an example of how to register a model, in the automated code, this is completed in register_model.py instead of train_pipeline.py"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "current accuracy 0.9833333333333334\n",
      "model is better\n",
      "Registering model iris-model\n",
      "Model properties add operation complete.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from azureml.core import Model\n",
    "from azureml.core.resource_configuration import ResourceConfiguration\n",
    "import sklearn\n",
    "\n",
    "model_name = 'iris-model'\n",
    "model_path = 'model_output'\n",
    "\n",
    "try:\n",
    "    model = Model(ws, model_name)\n",
    "    current_accuracy = float(model.properties[\"accuracy\"])\n",
    "except:\n",
    "    current_accuracy = 0\n",
    "\n",
    "print(\"current accuracy\",current_accuracy)\n",
    "if model_accuracy > current_accuracy:\n",
    "    print(\"model is better\")\n",
    "    model = Model.register(workspace=ws,\n",
    "                       model_name=model_name,                # Name of the registered model in your workspace.\n",
    "                       model_path=model_path,  # Local file to upload and register as a model.\n",
    "                       model_framework=Model.Framework.SCIKITLEARN,  # Framework used to create the model.\n",
    "                       model_framework_version=sklearn.__version__,  # Version of scikit-learn used to create the model.\n",
    "                       sample_input_dataset=dataset,\n",
    "                       #sample_output_dataset=output_dataset,\n",
    "                       resource_configuration=ResourceConfiguration(cpu=1, memory_in_gb=0.5),\n",
    "                       description='basic iris classification',\n",
    "                       tags={'quality': 'good', 'type': 'classification'})\n",
    "    model.add_properties({\"accuracy\":model_accuracy,\"model_type\":model_type})\n",
    "    model.experiment_name=experiment_name\n",
    "    model.run_id = run_id\n",
    "\n"
   ]
  },
  {
   "source": [
    "## Pipeline Publish and Schedule\n",
    "train_pipeline.py needs to be modified to perform publishing (change pipeline=True) to be published from the automated run. The below cells can be performed interactively after the pipeline above is submitted if desired"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Pipeline(Name: iris_training_demo,\n",
       "Id: f70ca185-1aa8-4b5b-91d5-33afb0204675,\n",
       "Status: Active,\n",
       "Endpoint: https://eastus2.api.azureml.ms/pipelines/v1.0/subscriptions/31e77061-7c45-4325-a0ec-1d348d195b23/resourceGroups/Synapse-WS-L400/providers/Microsoft.MachineLearningServices/workspaces/amlworkspacesjh/PipelineRuns/PipelineSubmit/f70ca185-1aa8-4b5b-91d5-33afb0204675)"
      ],
      "text/html": "<table style=\"width:100%\"><tr><th>Name</th><th>Id</th><th>Status</th><th>Endpoint</th></tr><tr><td>iris_training_demo</td><td><a href=\"https://ml.azure.com/pipelines/f70ca185-1aa8-4b5b-91d5-33afb0204675?wsid=/subscriptions/31e77061-7c45-4325-a0ec-1d348d195b23/resourcegroups/Synapse-WS-L400/workspaces/amlworkspacesjh\" target=\"_blank\" rel=\"noopener\">f70ca185-1aa8-4b5b-91d5-33afb0204675</a></td><td>Active</td><td><a href=\"https://eastus2.api.azureml.ms/pipelines/v1.0/subscriptions/31e77061-7c45-4325-a0ec-1d348d195b23/resourceGroups/Synapse-WS-L400/providers/Microsoft.MachineLearningServices/workspaces/amlworkspacesjh/PipelineRuns/PipelineSubmit/f70ca185-1aa8-4b5b-91d5-33afb0204675\" target=\"_blank\" rel=\"noopener\">REST Endpoint</a></td></tr></table>"
     },
     "metadata": {},
     "execution_count": 41
    }
   ],
   "source": [
    "published_pipeline = iris_train_pipeline.publish(name=\"iris_training_demo\", description=\"Iris Classification Demo\", continue_on_step_failure=True)\n",
    "published_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Provisioning status: Completed\n"
     ]
    }
   ],
   "source": [
    "from azureml.pipeline.core.schedule import ScheduleRecurrence, Schedule\n",
    "\n",
    "recurrence = ScheduleRecurrence(frequency=\"Day\", interval=1, hours=[22], minutes=[30]) # Runs every day at 10:30pm\n",
    "\n",
    "schedule = Schedule.create(workspace=ws, name=\"iris_training_demo_schedule\",\n",
    "                           pipeline_id=published_pipeline.id, \n",
    "                           experiment_name='iris_training_demo_daily_schedule_run',\n",
    "                           recurrence=recurrence,\n",
    "                           wait_for_provisioning=True,\n",
    "                           description=\"iris training demo daily Schedule Run\")"
   ]
  }
 ]
}